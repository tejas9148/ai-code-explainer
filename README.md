# ğŸ“¸ Project Screenshot

<p align="center">
  <img src="Screenshot 2026-02-17 230002.png" width="900"/>
</p>

# ğŸš€ AI Code Explainer

AI Code Explainer is a web-based application that explains programming code in simple and beginner-friendly terms using a Local Large Language Model (LLM).

The system runs fully offline using Ollama and does not require OpenAI or any external API.

---

# ğŸ“Œ Project Overview

This project allows users to:

- Paste source code (Python, Java, C++)
- Select programming language
- Get short AI-generated explanations
- Run completely locally using a local LLM

Tech Stack:

Frontend  â†’ React.js  
Backend   â†’ FastAPI  
AI Model  â†’ Ollama (TinyLlama / Phi3 Mini)  

---

# ğŸ—ï¸ Architecture

User (React UI)
        â†“
FastAPI Backend
        â†“
Ollama Local Model
        â†“
Generated Explanation
        â†“
Displayed on Frontend

---

# ğŸ› ï¸ Technologies Used

Frontend:
- React.js
- Axios
- CSS

Backend:
- Python 3.10+
- FastAPI
- Uvicorn
- Requests

AI:
- Ollama
- TinyLlama / Phi3 Mini

Version Control:
- Git
- GitHub

---

## ğŸ“‚ Project Structure

```
ai_code_explainer/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ venv/
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ frontend/
â”‚       â”œâ”€â”€ src/
â”‚       â”œâ”€â”€ public/
â”‚       â””â”€â”€ package.json
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```


# âš™ï¸ Installation Guide

Follow these steps to run the project locally.

---

## 1ï¸âƒ£ Install Python

Download:
https://www.python.org/downloads/

Check installation:
python --version

---

## 2ï¸âƒ£ Install Node.js

Download:
https://nodejs.org/

Check installation:
node -v  
npm -v

---

## 3ï¸âƒ£ Install Ollama

Download:
https://ollama.com

Verify:
ollama --version

---

## 4ï¸âƒ£ Download AI Model

Recommended for 8GB RAM:

ollama pull tinyllama

OR

ollama pull phi3:mini

Check installed models:
ollama list

---

# ğŸ”§ Backend Setup

Navigate to backend folder:

cd backend

Create virtual environment:

python -m venv venv

Activate (Windows):

venv\Scripts\activate

Install dependencies:

pip install fastapi uvicorn requests

(Optional) Generate requirements file:

pip freeze > requirements.txt

Run backend:

python -m uvicorn main:app --reload

Backend runs at:

http://127.0.0.1:8000

API Docs:

http://127.0.0.1:8000/docs

---

# ğŸ¨ Frontend Setup

Navigate to frontend folder:

cd frontend/frontend

Install dependencies:

npm install

Start React server:

npm start

Frontend runs at:

http://localhost:3000

---

# ğŸš€ Run Full Project

Open two terminals:

Terminal 1 (Backend):

cd backend  
venv\Scripts\activate  
python -m uvicorn main:app --reload  

Terminal 2 (Frontend):

cd frontend/frontend  
npm start  

Make sure Ollama is installed and model is downloaded.

---

# ğŸ“Œ How It Works

1. User enters code in React UI.
2. React sends POST request to FastAPI.
3. FastAPI creates prompt and sends it to Ollama.
4. Ollama generates explanation.
5. FastAPI returns explanation.
6. React displays output.

---

# ğŸ§  Prompt Design

Backend creates prompt like:

"Explain this code briefly in 3-4 lines..."

This ensures short and simple output.

---

# ğŸ“Š Features

- Short code explanations
- Clean dark UI
- Offline AI
- No API key required
- Beginner-friendly design
- Modern full-stack architecture

---

# âš ï¸ System Requirements

Minimum:
- 8GB RAM
- Python 3.10+
- Node.js
- Ollama installed

Note:
First response may be slow due to model loading.

---